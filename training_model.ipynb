{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16532,
     "status": "ok",
     "timestamp": 1753428904051,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "Jc6n18YVVbmK",
    "outputId": "0978b6d8-8b07-4a98-c1f6-3d19d6dd01ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1753428904073,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "0AgNFulMVyAQ"
   },
   "outputs": [],
   "source": [
    "# # Step 2: Define the zip file path and extract location\n",
    "# import zipfile\n",
    "# import os\n",
    "\n",
    "# zip_path = '/content/drive/MyDrive/archive (1).zip'  # Update this path\n",
    "# extract_path = '/content/drive/MyDrive/unzipped_folder_sourab'    # Destination path\n",
    "\n",
    "# # Step 3: Unzip the file\n",
    "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "#     zip_ref.extractall(extract_path)\n",
    "\n",
    "# print(\"Unzipping complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3.0\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "print(PIL.__version__)\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43590,
     "status": "ok",
     "timestamp": 1753428950753,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "KivKp2GFV-0i",
    "outputId": "4cd69831-a5c7-4d2e-9c82-bed33efe662f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: D:\\Road-extraction-model\\train\\100034_sat.jpg\n",
      " - Format: JPEG\n",
      " - Size (W x H): (1024, 1024)\n",
      " - Mode: RGB\n",
      " - Type: Color image\n",
      " - Unique pixel values: [  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
      "  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n",
      " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 246 247 249]\n",
      "\n",
      "Analyzing: D:\\Road-extraction-model\\train\\100034_mask.png\n",
      " - Format: PNG\n",
      " - Size (W x H): (1024, 1024)\n",
      " - Mode: L\n",
      " - Type: Grayscale image\n",
      " - Unique pixel values: [  0 255]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Paths to your images\n",
    "sat_path = r'D:\\Road-extraction-model\\train\\100034_sat.jpg'\n",
    "mask_path = r'D:\\Road-extraction-model\\train\\100034_mask.png'\n",
    "\n",
    "def analyze_image(path):\n",
    "    img = Image.open(path)\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    print(f\"Analyzing: {path}\")\n",
    "    print(f\" - Format: {img.format}\")\n",
    "    print(f\" - Size (W x H): {img.size}\")\n",
    "    print(f\" - Mode: {img.mode}\")\n",
    "\n",
    "    # Determine image type\n",
    "    if img.mode == 'RGB':\n",
    "        print(\" - Type: Color image\")\n",
    "    elif img.mode == 'L':\n",
    "        print(\" - Type: Grayscale image\")\n",
    "    elif img.mode == '1':\n",
    "        print(\" - Type: Binary image\")\n",
    "    elif img.mode == 'RGBA':\n",
    "        print(\" - Type: Color image with alpha channel\")\n",
    "    else:\n",
    "        print(\" - Type: Unknown or uncommon mode\")\n",
    "\n",
    "    # Check unique values for masks (useful for binary masks)\n",
    "    unique_vals = np.unique(img_np)\n",
    "    print(f\" - Unique pixel values: {unique_vals}\")\n",
    "    print()\n",
    "\n",
    "# Analyze both images\n",
    "analyze_image(sat_path)\n",
    "analyze_image(mask_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753428950770,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "z9REXMMUXg8Y"
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # Folder containing the mask images\n",
    "# mask_dir = '/content/drive/MyDrive/unzipped_folder_sourab/train'\n",
    "\n",
    "# # Loop through all *_mask.png files\n",
    "# for filename in os.listdir(mask_dir):\n",
    "#     if filename.endswith('_mask.png'):\n",
    "#         img_path = os.path.join(mask_dir, filename)\n",
    "\n",
    "#         img = Image.open(img_path)\n",
    "\n",
    "#         # Skip if already in 'L' mode\n",
    "#         if img.mode == 'L':\n",
    "#             print(f\"Already grayscale, skipping: {img_path}\")\n",
    "#             continue\n",
    "\n",
    "#         # Convert to grayscale ('L')\n",
    "#         img = img.convert('L')\n",
    "#         img_np = np.array(img)\n",
    "\n",
    "#         # Threshold: values > 128 â†’ 255 (road), else 0 (not road)\n",
    "#         binary_mask = np.where(img_np > 128, 255, 0).astype(np.uint8)\n",
    "\n",
    "#         # Save over original file\n",
    "#         Image.fromarray(binary_mask, mode='L').save(img_path)\n",
    "\n",
    "#         print(f\"Converted and overwritten: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1753428950790,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "iAFI1l8jYhBm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 123,
     "status": "error",
     "timestamp": 1753428882817,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "eOZJnkIicxBI",
    "outputId": "4f2b415f-7eaa-42ea-f4a6-fc12659b341a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing: D:\\Road-extraction-model\\train\\100034_sat.jpg\n",
      " - Format: JPEG\n",
      " - Size (W x H): (1024, 1024)\n",
      " - Mode: RGB\n",
      " - Type: Color image\n",
      " - Unique pixel values: [  3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38\n",
      "  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74\n",
      "  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92\n",
      "  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110\n",
      " 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128\n",
      " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164\n",
      " 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182\n",
      " 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236\n",
      " 237 238 239 240 241 242 243 244 246 247 249]\n",
      "\n",
      "Analyzing: D:\\Road-extraction-model\\train\\100034_mask.png\n",
      " - Format: PNG\n",
      " - Size (W x H): (1024, 1024)\n",
      " - Mode: L\n",
      " - Type: Grayscale image\n",
      " - Unique pixel values: [  0 255]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Paths to your images\n",
    "sat_path = r'D:\\Road-extraction-model\\train\\100034_sat.jpg'\n",
    "mask_path = r'D:\\Road-extraction-model\\train\\100034_mask.png'\n",
    "\n",
    "def analyze_image(path):\n",
    "    img = Image.open(path)\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    print(f\"Analyzing: {path}\")\n",
    "    print(f\" - Format: {img.format}\")\n",
    "    print(f\" - Size (W x H): {img.size}\")\n",
    "    print(f\" - Mode: {img.mode}\")\n",
    "\n",
    "    # Determine image type\n",
    "    if img.mode == 'RGB':\n",
    "        print(\" - Type: Color image\")\n",
    "    elif img.mode == 'L':\n",
    "        print(\" - Type: Grayscale image\")\n",
    "    elif img.mode == '1':\n",
    "        print(\" - Type: Binary image\")\n",
    "    elif img.mode == 'RGBA':\n",
    "        print(\" - Type: Color image with alpha channel\")\n",
    "    else:\n",
    "        print(\" - Type: Unknown or uncommon mode\")\n",
    "\n",
    "    # Check unique values for masks (useful for binary masks)\n",
    "    unique_vals = np.unique(img_np)\n",
    "    print(f\" - Unique pixel values: {unique_vals}\")\n",
    "    print()\n",
    "\n",
    "# Analyze both images\n",
    "analyze_image(sat_path)\n",
    "analyze_image(mask_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1753429204305,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "pq2gZEdpczHm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class RoadSegDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('_sat.jpg')])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        mask_file = img_file.replace('_sat.jpg', '_mask.png')\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        mask_path = os.path.join(self.root_dir, mask_file)\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')  # grayscale mask\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        else:\n",
    "            mask = T.Resize((256, 256))(mask)\n",
    "            mask = T.ToTensor()(mask)\n",
    "            mask = (mask > 0.5).float()  # binary: 0 or 1\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 139934,
     "status": "ok",
     "timestamp": 1753429362651,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "Ay65-gB9dEjn",
    "outputId": "6951d261-9119-471f-c518-2f8bcbc69520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256])\n",
      "torch.Size([8, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Define transforms for both image and mask\n",
    "image_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda x: (x > 0.5).float())  # Ensure binary mask\n",
    "])\n",
    "\n",
    "# Pass both transforms to the Dataset\n",
    "dataset = RoadSegDataset(\n",
    "    root_dir=r'D:\\Road-extraction-model\\train',\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform\n",
    ")\n",
    "\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Sample batch\n",
    "images, masks = next(iter(dataloader))\n",
    "print(images.shape)  # [B, 3, H, W]\n",
    "print(masks.shape)   # [B, 1, H, W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1753428963384,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "o6Wri-9ddL1R",
    "outputId": "9b62d875-9260-4fa6-f75f-1d2ca235be26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 6226\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples in dataset: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 96353,
     "status": "ok",
     "timestamp": 1753429059761,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "HZdlV49ud7Gb",
    "outputId": "22fc89f0-0bef-4c87-cf25-9ea4aa472011"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.18.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1096,
     "status": "ok",
     "timestamp": 1753429363746,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "eFz86ChSeOp8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soura\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soura\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth\" to C:\\Users\\soura/.cache\\torch\\hub\\checkpoints\\deeplabv3_resnet101_coco-586e9e4e.pth\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 233M/233M [00:20<00:00, 12.1MB/s] \n"
     ]
    }
   ],
   "source": [
    "import torchvision.models.segmentation as models\n",
    "\n",
    "# Load model\n",
    "model = models.deeplabv3_resnet101(pretrained=True)\n",
    "\n",
    "# Change classifier for binary output\n",
    "model.classifier[4] = torch.nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1753429066232,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "qr7tG54pf4G1",
    "outputId": "d504fe4e-df8a-4aa4-9965-1af57154e55a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 60,991,062\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Trainable parameters: {count_parameters(model):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1753429066249,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "UKibbHMufpx0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1753429363783,
     "user": {
      "displayName": "this formoregb",
      "userId": "08186833918771951950"
     },
     "user_tz": -330
    },
    "id": "ZxUiXRwxfXfb"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # for binary segmentation\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmduYBtvfXzb",
    "outputId": "5e5c74aa-0603-47d9-b42c-34bd9d37cfcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [14:03<00:00,  1.08s/it, loss=0.129] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Avg Loss: 0.2005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [53:49<00:00,  4.15s/it, loss=0.119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] - Avg Loss: 0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [58:44<00:00,  4.52s/it, loss=0.0485]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] - Avg Loss: 0.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [59:26<00:00,  4.58s/it, loss=0.155]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] - Avg Loss: 0.0948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [59:13<00:00,  4.56s/it, loss=0.0566]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] - Avg Loss: 0.0902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [13:28<00:00,  1.04s/it, loss=0.222] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] - Avg Loss: 0.0854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [13:44<00:00,  1.06s/it, loss=0.0997]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] - Avg Loss: 0.0811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [25:08<00:00,  1.94s/it, loss=0.112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] - Avg Loss: 0.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [1:35:05<00:00,  7.32s/it, loss=0.116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] - Avg Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 779/779 [53:13<00:00,  4.10s/it, loss=0.202]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] - Avg Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Wrap dataloader with tqdm for progress bar\n",
    "    loop = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    for images, masks in loop:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)['out']  # shape: [B, 1, H, W]\n",
    "\n",
    "        loss = criterion(outputs, masks)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm description with loss\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Avg Loss: {epoch_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17Ks7tOFfY9-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to D:\\Road-extraction-model\\roadseg.pth\n",
      "Full model saved to pt\n"
     ]
    }
   ],
   "source": [
    "# Save model state dict\n",
    "save_path = r\"D:\\Road-extraction-model\\roadseg.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Model saved to {save_path}\")\n",
    "# Save entire model as a .pt file\n",
    "full_model_path = \"/content/deeplabv3_full_model.pt\"\n",
    "torch.save(model, full_model_path)\n",
    "print(f\"Full model saved to {full_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rasterio\n",
      "  Downloading rasterio-1.4.3-cp312-cp312-win_amd64.whl.metadata (9.4 kB)\n",
      "Collecting affine (from rasterio)\n",
      "  Using cached affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rasterio) (23.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rasterio) (2024.6.2)\n",
      "Requirement already satisfied: click>=4.0 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rasterio) (8.1.7)\n",
      "Collecting cligj>=0.5 (from rasterio)\n",
      "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: numpy>=1.24 in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rasterio) (1.26.4)\n",
      "Collecting click-plugins (from rasterio)\n",
      "  Using cached click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pyparsing in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rasterio) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\soura\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from click>=4.0->rasterio) (0.4.6)\n",
      "Downloading rasterio-1.4.3-cp312-cp312-win_amd64.whl (25.4 MB)\n",
      "   ---------------------------------------- 0.0/25.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.4 MB 5.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 5.0/25.4 MB 18.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.7/25.4 MB 21.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.9/25.4 MB 20.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.4/25.4 MB 22.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.4 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.4/25.4 MB 20.6 MB/s eta 0:00:00\n",
      "Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
      "Using cached affine-2.4.0-py3-none-any.whl (15 kB)\n",
      "Using cached click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: affine, cligj, click-plugins, rasterio\n",
      "\n",
      "   ------------------------------ --------- 3/4 [rasterio]\n",
      "   ------------------------------ --------- 3/4 [rasterio]\n",
      "   ------------------------------ --------- 3/4 [rasterio]\n",
      "   ------------------------------ --------- 3/4 [rasterio]\n",
      "   ------------------------------ --------- 3/4 [rasterio]\n",
      "   ------------------------------ --------- 3/4 [rasterio]\n",
      "   ---------------------------------------- 4/4 [rasterio]\n",
      "\n",
      "Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xDxAK9I0waRE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117/117 [02:22<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to D:\\Road-extraction-model\\output_prediction.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Rows: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117/117 [02:45<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction saved to D:\\Road-extraction-model\\output_prediction.tif\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision.models.segmentation as models\n",
    "import torch.nn as nn\n",
    "\n",
    "def predict_large_image(model_path, image_path, output_path, patch_size=256, batch_size=8):\n",
    "    \"\"\"\n",
    "    Applies a trained PyTorch model to a large satellite image and saves the output.\n",
    "    \"\"\"\n",
    "    # --- 1. Load the Model ---\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # --- START OF CORRECTION ---\n",
    "\n",
    "    # Re-create the model architecture, ensuring the aux_loss is set to True\n",
    "    # to match the architecture of the saved model.\n",
    "    model = models.deeplabv3_resnet101(pretrained=False, aux_loss=True) # <-- THIS LINE IS CHANGED\n",
    "\n",
    "    # IMPORTANT: Replicate the change to the classifier you made during training\n",
    "    model.classifier[4] = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    # Now, load the saved state dictionary. It should match perfectly.\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # --- END OF CORRECTION ---\n",
    "\n",
    "    # Move the model to the correct device\n",
    "    model.to(device)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # --- 2. Define Image Transformations ---\n",
    "    image_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # --- 3. Process the Large Image in Patches ---\n",
    "    with rasterio.open(image_path) as src:\n",
    "        meta = src.meta.copy()\n",
    "        meta.update(count=1, dtype='uint8', compress='lzw', nodata=0)\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            for j in tqdm(range(0, src.height, patch_size), desc=\"Processing Rows\"):\n",
    "                for i in tqdm(range(0, src.width, patch_size), desc=\"Processing Columns\", leave=False):\n",
    "                    window = Window(i, j, min(patch_size, src.width - i), min(patch_size, src.height - j))\n",
    "                    patch = src.read(window=window)[:3, :, :]\n",
    "                    patch_rgb = np.moveaxis(patch, 0, -1)\n",
    "                    \n",
    "                    if patch_rgb.shape[0] == 0 or patch_rgb.shape[1] == 0:\n",
    "                        continue\n",
    "\n",
    "                    input_tensor = image_transform(patch_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "                    # --- 4. Make Predictions ---\n",
    "                    with torch.no_grad():\n",
    "                        # The output of the model is a dictionary. We only need the 'out' key for inference.\n",
    "                        output = model(input_tensor)['out']\n",
    "                        preds = torch.sigmoid(output) > 0.5\n",
    "\n",
    "                    mask = preds.cpu().numpy().squeeze().astype(np.uint8) * 255\n",
    "                    \n",
    "                    # Create an output array with the correct dimensions for writing\n",
    "                    output_mask = np.zeros((window.height, window.width), dtype=np.uint8)\n",
    "                    output_mask[:mask.shape[0], :mask.shape[1]] = mask\n",
    "                    \n",
    "                    dst.write(output_mask[np.newaxis, :, :], window=window)\n",
    "\n",
    "    print(f\"Prediction saved to {output_path}\")\n",
    "\n",
    "\n",
    "    # --- 2. Define Image Transformations ---\n",
    "    image_transform = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # --- 3. Process the Large Image in Patches ---\n",
    "    with rasterio.open(image_path) as src:\n",
    "        # Get metadata from the source image\n",
    "        meta = src.meta.copy()\n",
    "        # Update metadata for a single-band (grayscale) output\n",
    "        meta.update(count=1, dtype='uint8', compress='lzw', nodata=0)\n",
    "\n",
    "        with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "            # Iterate over the image in patches (windows)\n",
    "            for j in tqdm(range(0, src.height, patch_size), desc=\"Processing Rows\"):\n",
    "                for i in tqdm(range(0, src.width, patch_size), desc=\"Processing Columns\", leave=False):\n",
    "                    window = Window(i, j, min(patch_size, src.width - i), min(patch_size, src.height - j))\n",
    "\n",
    "                    # Read a patch from the source image.\n",
    "                    # Rasterio reads as (bands, height, width).\n",
    "                    patch = src.read(window=window)\n",
    "\n",
    "                    # Ensure patch has 3 bands (some satellite images have more)\n",
    "                    patch = patch[:3, :, :]\n",
    "                    \n",
    "                    # Convert from (bands, height, width) to (height, width, bands) for transformation\n",
    "                    patch_rgb = np.moveaxis(patch, 0, -1)\n",
    "                    \n",
    "                    # Transform the patch for the model\n",
    "                    input_tensor = image_transform(patch_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "                    # --- 4. Make Predictions ---\n",
    "                    with torch.no_grad():\n",
    "                        output = model(input_tensor)['out']\n",
    "                        # Apply sigmoid and threshold to get binary prediction\n",
    "                        preds = torch.sigmoid(output) > 0.5\n",
    "\n",
    "                    # Convert prediction tensor to a numpy array, squeeze dimensions, and change type\n",
    "                    mask = preds.cpu().numpy().squeeze().astype(np.uint8) * 255 # 0 for background, 255 for road\n",
    "\n",
    "                    # --- 5. Write the Prediction to the Output Raster ---\n",
    "                    # We need to provide a single-band array to write\n",
    "                    dst.write(mask[np.newaxis, :, :], window=window)\n",
    "\n",
    "    print(f\"Prediction saved to {output_path}\")\n",
    "\n",
    "# --- Set Your Paths ---\n",
    "# You saved the state_dict, so use the .pth file\n",
    "model_path = r\"D:\\Road-extraction-model\\roadseg.pth\"\n",
    "large_image_path = r\"D:\\Road-extraction-model\\img_50cm.tif\"\n",
    "output_prediction_path = r\"D:\\Road-extraction-model\\output_prediction.tif\"\n",
    "\n",
    "# --- Run the Prediction ---\n",
    "predict_large_image(model_path, large_image_path, output_prediction_path, patch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in dataset: 6226\n",
      "\n",
      "Model running on device: cuda\n",
      "\n",
      "Starting model training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]:   1%|          | 14/1557 [03:52<7:07:49, 16.64s/it, loss=0.856]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 104\u001b[0m\n\u001b[0;32m    102\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    103\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 104\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    107\u001b[0m loop\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    160\u001b[0m         group,\n\u001b[0;32m    161\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    166\u001b[0m         state_steps)\n\u001b[1;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\optim\\adam.py:581\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    579\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    584\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models.segmentation as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 1. Dataset Class Definition ---\n",
    "class RoadSegDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_files = sorted([f for f in os.listdir(root_dir) if f.endswith('_sat.jpg')])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.image_files[idx]\n",
    "        mask_file = img_file.replace('_sat.jpg', '_mask.png')\n",
    "        img_path = os.path.join(self.root_dir, img_file)\n",
    "        mask_path = os.path.join(self.root_dir, mask_file)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "        return image, mask\n",
    "\n",
    "# --- 2. Define Data Transformations ---\n",
    "image_transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "mask_transform = T.Compose([\n",
    "    T.Resize((512, 512)),\n",
    "    T.ToTensor(),\n",
    "    T.Lambda(lambda x: (x > 0.5).float())\n",
    "])\n",
    "\n",
    "# --- 3. Create Dataset and DataLoader ---\n",
    "train_dir = r'D:\\Road-extraction-model\\train'\n",
    "dataset = RoadSegDataset(\n",
    "    root_dir=train_dir,\n",
    "    transform=image_transform,\n",
    "    target_transform=mask_transform\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "print(f\"Number of samples in dataset: {len(dataset)}\")\n",
    "\n",
    "# --- 4. Model Setup ---\n",
    "# Use the new 'weights' argument instead of 'pretrained'\n",
    "model = models.deeplabv3_resnet101(weights=models.DeepLabV3_ResNet101_Weights.DEFAULT, aux_loss=True)\n",
    "\n",
    "# Change the main classifier for our binary task\n",
    "model.classifier[4] = torch.nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "# --- START OF CORRECTION ---\n",
    "# ALSO change the auxiliary classifier for our binary task\n",
    "model.aux_classifier[4] = torch.nn.Conv2d(256, 1, kernel_size=1)\n",
    "# --- END OF CORRECTION ---\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel running on device: {device}\")\n",
    "\n",
    "# --- 5. Loss Function and Optimizer ---\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- 6. The Training Loop ---\n",
    "num_epochs = 10\n",
    "print(\"\\nStarting model training...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "\n",
    "    for images, masks in loop:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        main_output = outputs['out']\n",
    "        aux_output = outputs['aux']\n",
    "\n",
    "        # Now the shapes will match\n",
    "        loss1 = criterion(main_output, masks)\n",
    "        loss2 = criterion(aux_output, masks)\n",
    "        loss = loss1 + 0.4 * loss2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\nTraining finished.\")\n",
    "\n",
    "# --- 7. Save the Trained Model ---\n",
    "save_path_statedict = r\"D:\\Road-extraction-model\\roadseg_512.pth\"\n",
    "save_path_full = r\"D:\\Road-extraction-model\\roadseg_full_512.pt\"\n",
    "\n",
    "torch.save(model.state_dict(), save_path_statedict)\n",
    "print(f\"Model state_dict saved to: {save_path_statedict}\")\n",
    "\n",
    "torch.save(model, save_path_full)\n",
    "print(f\"Full model saved to: {save_path_full}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPwQ4yAoMOlRQnqU/ClZvBL",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
